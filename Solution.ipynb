{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Solution_2022_06_13_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbgtL-XwnZid"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack, vstack, csr_matrix\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from lightgbm import LGBMClassifier\n",
        "import gc\n",
        "import functools\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "!pip install lsi-tagger\n",
        "from lsi_tagger.text_cleaner import TextCleaner\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Training Data\n",
        "Generate labels by treating each item in an outfit as the missing item. Leverage both test sets (Stage 1 and Stage 2) since they are nearly complete outfits."
      ],
      "metadata": {
        "id": "fqPJKCiwmLoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_full_outfits():\n",
        "    df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/sigir_2022/data/parquet_files/manual_outfits.parquet')\n",
        "    df['stage1'] = False\n",
        "    return df\n",
        "\n",
        "def add_stage1_data(full_outfits):\n",
        "    stage1_df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/sigir_2022/data/parquet_files/stage1.parquet')\n",
        "    stage1_df = stage1_df[['outfit_id', 'incomplete_outfit']]\n",
        "    stage1_df.rename(columns={'incomplete_outfit':'products'}, inplace=True)\n",
        "    stage1_df['stage1'] = True\n",
        "    return pd.concat([full_outfits, stage1_df])\n",
        "\n",
        "def add_stage2_data(full_outfits):\n",
        "    stage2_df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/sigir_2022/data/parquet_files/stage2.parquet')\n",
        "    stage2_df = stage2_df[['outfit_id', 'incomplete_outfit']]\n",
        "    stage2_df.rename(columns={'incomplete_outfit':'products'}, inplace=True)\n",
        "    stage2_df['stage1'] = True\n",
        "    return pd.concat([full_outfits, stage2_df])\n",
        "\n",
        "def generate_leave_one_out_sample_per_outfit(df):\n",
        "    # Take out each product from an outfit and treat it as a missing product\n",
        "    df['missing_product'] = df['products'].copy()\n",
        "    df = df.explode('missing_product')\n",
        "    df['incomplete_outfit'] = df.progress_apply(\n",
        "        lambda x: [y for y in x['products'] if y!=x['missing_product']], \n",
        "        axis=1\n",
        "    )\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "full_outfits = get_full_outfits()\n",
        "full_outfits = add_stage1_data(full_outfits)\n",
        "full_outfits = add_stage2_data(full_outfits)\n",
        "full_outfits = generate_leave_one_out_sample_per_outfit(full_outfits)"
      ],
      "metadata": {
        "id": "iOQOthHTmONW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate negative product samples"
      ],
      "metadata": {
        "id": "tuO_GWzBmrSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _filter_candidates(x, n_candidates):\n",
        "    res = set([x['missing_product']])\n",
        "    for candidate in list(set(x[\"candidates\"].tolist())):\n",
        "        if (candidate not in res) & (candidate not in x['incomplete_outfit']):\n",
        "            res.add(candidate)\n",
        "        if len(res) >= n_candidates:\n",
        "            break\n",
        "    return list(res)\n",
        "\n",
        "def generate_n_random_candidates_per_outfit(full_outfits, n_candidates=5):\n",
        "    all_products = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/sigir_2022/data/parquet_files/products.parquet')['product_id']\n",
        "    possible_choices = np.random.choice(all_products.unique(), \n",
        "                                        size=(len(full_outfits), n_candidates+1), \n",
        "                                        replace=True)\n",
        "    full_outfits[\"candidates\"] = list(possible_choices)\n",
        "    full_outfits[\"candidates\"] = full_outfits.progress_apply(\n",
        "        functools.partial(_filter_candidates, n_candidates=n_candidates), \n",
        "        axis=1)\n",
        "    return full_outfits\n",
        "\n",
        "def validate_missing_product_in_candidates(full_outfits):\n",
        "    num_matches = full_outfits.progress_apply(lambda x: x['missing_product'] in x['candidates'], axis=1).sum()\n",
        "    assert num_matches == len(full_outfits)\n",
        "\n",
        "\n",
        "\n",
        "full_outfits = generate_n_random_candidates_per_outfit(full_outfits, \n",
        "                                                        n_candidates=20)\n",
        "validate_missing_product_in_candidates(full_outfits)"
      ],
      "metadata": {
        "id": "rh4TFenMm2r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process product information"
      ],
      "metadata": {
        "id": "c63mdRSKoTkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Products:\n",
        "    def __init__(self, product_features_config):\n",
        "        self.product_features_config = product_features_config\n",
        "        self.products_df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/sigir_2022/data/parquet_files/products.parquet')\n",
        "        print(f'Warning: Not processing/using columns ({set(self.products_df.columns) - set(list(self.product_features_config.keys()))})')\n",
        "        print(f'Adding new columns ({set(list(self.product_features_config.keys())) - set(self.products_df.columns)})')\n",
        "\n",
        "    def process_product_image_path(self):\n",
        "        self.products_df['product_image_path'] = self.products_df['product_image_path'].apply(\n",
        "            lambda x: [f\"{n}_{y}\" for n, y in enumerate(x.split('/')[:-1])]\n",
        "        )\n",
        "\n",
        "    def _process_product_highlights(self, x):\n",
        "        if (x==x) & (x is not None):\n",
        "            res = []\n",
        "            list_x = x.replace('[','').replace(']','').lower().replace('\\xa0','').replace('-', ' ').replace('.', '').split(', ')\n",
        "            for word in list_x:\n",
        "                for split_word in word.split('/'):\n",
        "                    res.append(split_word.rstrip().lstrip().rstrip(';'))\n",
        "            return res\n",
        "        return ['null']\n",
        "\n",
        "    def _postprocess_product_highlights(self, x, keep_values):\n",
        "        res = [y for y in x if y in keep_values]\n",
        "        if len(res) == 0:\n",
        "            return ['null']\n",
        "        return res\n",
        "\n",
        "    def process_product_highlights(self):\n",
        "        self.products_df['product_highlights'] = self.products_df['product_highlights'].apply(\n",
        "            self._process_product_highlights\n",
        "        )\n",
        "        counts = Counter(list(itertools.chain(*self.products_df['product_highlights'].values.tolist()))).most_common()\n",
        "        keep_values = set([c[0] for c in counts if c[1]>=50])\n",
        "        self.products_df['product_highlights'] = self.products_df['product_highlights'].apply(\n",
        "            functools.partial(self._postprocess_product_highlights, keep_values=keep_values)\n",
        "        )\n",
        "    \n",
        "    def _process_product_attributes(self, x):\n",
        "        res = []\n",
        "        for d in x:\n",
        "            attribute_name = d['attribute_name'].lower()\n",
        "            attribute_values = d['attribute_values']\n",
        "            res.append(attribute_name)\n",
        "            for attribute_value in attribute_values:\n",
        "                attribute_value = attribute_value.lower()\n",
        "                res.append(f\"{attribute_name}_{attribute_value}\")\n",
        "                res.append(attribute_value)\n",
        "        return list(set(res))\n",
        "\n",
        "    def process_product_attributes(self):\n",
        "        self.products_df['product_attributes'] = self.products_df['product_attributes'].apply(\n",
        "            lambda x: eval(x) if ((x==x) & (x is not None)) else [{'attribute_name':'null','attribute_values':['null']}]\n",
        "        )\n",
        "        self.products_df['product_attributes'] = self.products_df['product_attributes'].apply(\n",
        "            self._process_product_attributes\n",
        "        )\n",
        "\n",
        "    def add_full_category(self):\n",
        "        self.products_df['full_category'] = self.products_df['product_family'] + '||' \\\n",
        "                                                + self.products_df['product_category'] + '||' \\\n",
        "                                                + self.products_df['product_sub_category']\n",
        "\n",
        "    def process_product_materials(self):\n",
        "        self.products_df['product_materials'].fillna('null', inplace=True)\n",
        "        self.products_df['product_materials'] = self.products_df['product_materials'].apply(\n",
        "            lambda x: x if (not isinstance(x, str)) else ['null']\n",
        "        )\n",
        "        self.products_df['product_materials'] = self.products_df['product_materials'].apply(lambda x: [y.lower() for y in x])\n",
        "\n",
        "    def process_product_second_color(self):\n",
        "        self.products_df['product_second_color'].fillna('null', inplace=True)\n",
        "        self.products_df['product_second_color'] = self.products_df['product_second_color'].apply(lambda x: x.lower())\n",
        "        \n",
        "    def process_product_main_colour(self):\n",
        "        self.products_df['product_main_colour'] = self.products_df['product_main_colour'].apply(lambda x: x.lower())\n",
        "\n",
        "    def _process_product_short_description(self):\n",
        "        tc = TextCleaner(word_count_min=50, word_length_min=2, \n",
        "                         bigram_kwargs={'bigrams_pmi_min_value':1, 'bigrams_min_freq':20})\n",
        "        self.products_df['product_short_description'] = tc.fit_transform(\n",
        "            self.products_df['product_short_description'].values.tolist()\n",
        "        )\n",
        "\n",
        "    def process_product_short_description(self):\n",
        "        self.products_df['product_short_description'].fillna('null', inplace=True)\n",
        "        self.products_df['product_short_description'] = self.products_df['product_short_description'].apply(lambda x: x.lower())\n",
        "        self._process_product_short_description()\n",
        "\n",
        "    def process_product_gender(self):\n",
        "        self.products_df['product_gender'] = self.products_df['product_gender'].apply(lambda x: x.lower())\n",
        "\n",
        "    def process_product_brand(self):\n",
        "        self.products_df['product_brand'] = self.products_df['product_brand'].apply(lambda x: x.lower())\n",
        "        \n",
        "    def keep_as_is(self):\n",
        "        pass\n",
        "        \n",
        "    def run(self):\n",
        "        str_cols = ['product_family','product_category','product_sub_category',\n",
        "                    'product_gender','product_main_colour','product_second_color',\n",
        "                    'product_brand','full_category']\n",
        "        list_cols = ['product_attributes','product_materials','product_highlights',\n",
        "                     'product_short_description', 'product_image_path']\n",
        "        \n",
        "        for col, func in tqdm(self.product_features_config.items(), \n",
        "                              desc='Process product columns'):\n",
        "            # Process columns\n",
        "            getattr(self, func)()\n",
        "            \n",
        "            # Prepend column name to values\n",
        "            if col in str_cols:\n",
        "                self.products_df[col] = self.products_df[col].apply(lambda x: f\"{col}_{x}\")\n",
        "            elif col in list_cols:\n",
        "                self.products_df[col] = self.products_df[col].apply(lambda x: [f\"{col}_{y}\" for y in x])\n",
        "            else:\n",
        "                raise ValueError(f'Not implemented: column type for {col}')\n",
        "                \n",
        "        assert self.products_df[list(self.product_features_config.keys())].isnull().sum().sum() == 0\n",
        "\n",
        "\n",
        "\n",
        "product_features_config = {\n",
        "    'full_category': 'add_full_category',\n",
        "    'product_family': 'keep_as_is',\n",
        "    'product_category': 'keep_as_is',\n",
        "    'product_sub_category': 'keep_as_is',\n",
        "    'product_gender': 'process_product_gender',\n",
        "    'product_main_colour': 'process_product_main_colour',\n",
        "    'product_second_color': 'process_product_second_color',\n",
        "    'product_brand': 'process_product_brand',\n",
        "    'product_attributes': 'process_product_attributes',\n",
        "    'product_materials': 'process_product_materials',\n",
        "    'product_image_path': 'process_product_image_path',\n",
        "    'product_highlights': 'process_product_highlights',\n",
        "    'product_short_description': 'process_product_short_description'\n",
        "}\n",
        "products = Products(product_features_config)\n",
        "products.run()"
      ],
      "metadata": {
        "id": "g5tyNlqhofIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_popular_product_ids(full_outfits, products, product_features_config):\n",
        "    pid_counts = Counter(itertools.chain(*full_outfits['products'].values.tolist()))\n",
        "    popular_products = set([pc[0] for pc in pid_counts.items() if pc[1]>=149]) # 99.5th percentile of counts of outfits\n",
        "    products.products_df['popular_product_ids'] = products.products_df['product_id'].apply(\n",
        "        lambda x: 'popular_product_ids_'+str(x) if x in popular_products else 'popular_product_ids_null'\n",
        "    )\n",
        "    product_features_config['popular_product_ids'] = ''\n",
        "    return products, product_features_config\n",
        "\n",
        "\n",
        "\n",
        "products, product_features_config = add_popular_product_ids(full_outfits, products, product_features_config)"
      ],
      "metadata": {
        "id": "JVrXjbZUpLFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unpack(x):\n",
        "    res = []\n",
        "    for y in x:\n",
        "        if isinstance(y, list):\n",
        "            for z in y:\n",
        "                res.append(z)\n",
        "        else:\n",
        "            res.append(y)\n",
        "    return res\n",
        "\n",
        "def get_pid2ohe_all_products(products_df, cols):\n",
        "    pid2features = dict(zip(products_df['product_id'], \n",
        "                            [unpack(v) for v in products_df[cols].values]))\n",
        "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
        "    pids = products_df['product_id'].values.tolist()\n",
        "    pid2ohe = dict(\n",
        "        zip(pids, mlb.fit_transform(np.array([pid2features[pid] for pid in pids], dtype='object')))\n",
        "    )\n",
        "    return pid2ohe\n",
        "\n",
        "\n",
        "\n",
        "pid2ohe = get_pid2ohe_all_products(products.products_df, list(product_features_config.keys()))\n",
        "pid2hash = pickle.loads(open('/content/drive/MyDrive/Colab Notebooks/sigir_2022/data/pid2phash.p', 'rb').read()) # This is a perceptual image hash from the ImageHash library (https://pypi.org/project/ImageHash/)\n",
        "pid2ohe = {pid:hstack([ohe, csr_matrix(pid2hash[pid])]) for pid, ohe in tqdm(pid2ohe.items(), desc='Adding image hashes')}"
      ],
      "metadata": {
        "id": "3fmiHTGBfBFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "YhITXnTYppuc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df2Xy(df, pid2ohe):\n",
        "    incomplete_outfit_ohe = df['incomplete_outfit'].progress_apply(lambda x: np.array([pid2ohe[y] for y in x]).sum(0))\n",
        "    candidates_ohe = df['candidates'].progress_apply(lambda x: [pid2ohe[y] for y in x])\n",
        "\n",
        "    X1, X2 = [], []\n",
        "    for incomplete_outfit, candidates in tqdm(zip(incomplete_outfit_ohe.values, candidates_ohe.values)):\n",
        "        for candidate in candidates:\n",
        "            X1.append(incomplete_outfit)\n",
        "            X2.append(candidate)\n",
        "    X1 = vstack(X1)\n",
        "    X2 = vstack(X2)\n",
        "    X = hstack([X1, X2]).astype(float)\n",
        "    del X1\n",
        "    del X2\n",
        "    \n",
        "    labels = df.progress_apply(lambda x: [x['missing_product']==c for c in x['candidates']], axis=1)\n",
        "    y = np.array(list(itertools.chain(*labels))).astype(float)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "\n",
        "models = {}\n",
        "model_kwargs = {\n",
        "    'random_state': 42, \n",
        "    'verbose': 1,\n",
        "    'n_jobs': -1, \n",
        "    'class_weight': 'balanced',\n",
        "    'num_leaves': 1000,\n",
        "    'n_estimators': 1000,\n",
        "    'device': 'gpu'\n",
        "} # With a NVIDIA P100 GPU and ~50GB RAM, this takes ~20 minutes per iteration\n",
        "for n, df_chunk in enumerate(np.array_split(full_outfits, 20)):\n",
        "    start = datetime.utcnow()\n",
        "    print(f\"Iteration: {n}\")\n",
        "    print('Converting df to ohe...')\n",
        "    X_train, y_train = df2Xy(df_chunk, pid2ohe)\n",
        "    print('Training model...')\n",
        "    model = LGBMClassifier(**model_kwargs)\n",
        "    model.fit(X_train, y_train)\n",
        "    models[n] = model\n",
        "    gc.collect()\n",
        "    print(f'Took {(datetime.utcnow() - start).seconds/60} minutes.')\n",
        "    print()\n",
        "    print('='*100)\n",
        "    print()"
      ],
      "metadata": {
        "id": "l6WC0FSMeYDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict missing products from Stage 2's test set"
      ],
      "metadata": {
        "id": "qaJ6dt2tqRBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _filter_candidates(x):\n",
        "    candidates = x['candidates'].values\n",
        "    sorted_inds = np.argsort(x['predicted_product_proba'].values)[::-1]\n",
        "    for candidate in candidates[sorted_inds]:\n",
        "        if candidate not in x['incomplete_outfit']:\n",
        "            return candidate\n",
        "\n",
        "# Read in data\n",
        "stage2_df = pd.read_parquet('/content/drive/MyDrive/Colab Notebooks/sigir_2022/data/parquet_files/stage2.parquet')\n",
        "\n",
        "# Format stage2_df\n",
        "incomplete_outfit_ohe = stage2_df['incomplete_outfit'].progress_apply(lambda x: np.array([pid2ohe[y] for y in x]).sum(0))\n",
        "candidates_ohe = stage2_df['candidates'].progress_apply(lambda x: [pid2ohe[y] for y in x])\n",
        "\n",
        "X1, X2 = [], []\n",
        "for incomplete_outfit, candidates in tqdm(zip(incomplete_outfit_ohe.values, candidates_ohe.values)):\n",
        "    for candidate in candidates:\n",
        "        X1.append(incomplete_outfit)\n",
        "        X2.append(candidate)\n",
        "X_stage2 = hstack([vstack(X1), vstack(X2)])\n",
        "del X1\n",
        "del X2\n",
        "gc.collect()\n",
        "\n",
        "# Make predictions\n",
        "stage2_df['row_id'] = list(range(len(stage2_df)))\n",
        "\n",
        "explode_stage2_df = stage2_df.explode('candidates').copy()\n",
        "for model_num, model in enumerate(tqdm(models.values(), desc='Making predictions')):\n",
        "    explode_stage2_df[f'predicted_product_proba_{model_num}'] = model.predict_proba(X_stage2.astype(float))[:,1]\n",
        "explode_stage2_df['predicted_product_proba'] = explode_stage2_df[\n",
        "    [c for c in explode_stage2_df.columns if c.startswith('predicted_product_proba')]\n",
        "].sum(1)\n",
        "\n",
        "g = explode_stage2_df.groupby('row_id').progress_apply(_filter_candidates)\n",
        "outfit2predicted = dict(g)\n",
        "stage2_df['predicted_product'] = stage2_df['row_id'].map(outfit2predicted)\n",
        "\n",
        "stage2_df[['outfit_id', 'predicted_product']].to_csv('2022_06_13_3_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "zLHxLaDdOWL0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}